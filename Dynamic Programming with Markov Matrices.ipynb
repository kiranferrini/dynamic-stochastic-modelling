{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d3367914",
   "metadata": {},
   "source": [
    "# Homework 5\n",
    "### Joe Schneider, Kiran Ferrini, Mark Armstrong"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1f041b3",
   "metadata": {},
   "source": [
    "# Question 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff5fd3c",
   "metadata": {},
   "source": [
    "The five elements of dynamic programming are:\n",
    "\n",
    "1. The State Space:  $𝑋 = \\{\\text{Bad Year,  Normal Year,  Good Year}\\}$\n",
    "2. The Control Set:  $𝑈 =  \\{\\text{Ad Buy,  No Ad Buy}\\}$\n",
    "3. The Reward Function:  $𝑟(𝑥,𝑢)$\n",
    "\n",
    ">{Good Year, Ad Buy} $\\Rightarrow$  \\\\$3 Million  \n",
    "{Good Year, No Ad Buy} $\\Rightarrow$ \\\\$4 Million    \n",
    "{Normal Year, Ad Buy} $\\Rightarrow$  \\\\$1 Million  \n",
    "{Normal Year, No Ad Buy} $\\Rightarrow$ \\\\$2 Million   \n",
    "{Bad Year, Ad Buy} $\\Rightarrow$ \\\\$0  \n",
    "{Bad Year, No Ad Buy} $\\Rightarrow$ \\\\$1 Million  \n",
    "\n",
    "4. A Stochastic Transition Rule for the Distribution of Next Period's State:  $𝑥_{𝑡+1}∼𝑔(𝑥_𝑡,𝑢_𝑡)$\n",
    "\n",
    ">$\\begin{aligned}\n",
    "M(\\text{No Ad Buy}) = \n",
    "\\begin{bmatrix}\n",
    "0.8 & 0.2 & 0\\\\\n",
    "0.2 & 0.8 & 0 \\\\\n",
    "0.1 & 0.4 & 0.5\n",
    "\\end{bmatrix}\n",
    "\\;\\;\\;\\;\\;\\; \\text{and} \\;\\;\\;\\;\\;\\;\n",
    "M(\\text{Ad Buy}) = \n",
    "\\begin{bmatrix}\n",
    "0.5 & 0.5 & 0\\\\\n",
    "0.1 & 0.6 & 0.3 \\\\\n",
    "0.1 & 0.3 & 0.6\n",
    "\\end{bmatrix}\\end{aligned}$\n",
    " \n",
    "5. The Discount Factor: 𝛽∈[0,1)\n",
    "\n",
    ">$\\begin{aligned}\n",
    "\\beta = \\frac{1}{1+r}\n",
    "\\end{aligned}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "992db917",
   "metadata": {},
   "source": [
    "# Question 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e92b30",
   "metadata": {},
   "source": [
    "Bellman Equation:\n",
    "\n",
    ">$\\begin{aligned}\n",
    "V(x_t) = \\max_{u_t \\in U} \\;\\; {r(x_t,\\;u_t)+\\beta \\cdot E [\\;V(x_{t+1})\\;|\\;x_t]}\n",
    "\\end{aligned}\n",
    "$\n",
    "\n",
    "For the company in a **Normal Year**:\n",
    "\n",
    ">$\\begin{aligned}\n",
    "V(\\text{Normal}) = \\max \\;\\; \\{\\;2 \\;+\\;  \\frac{1}{1+r} \\; \\cdot \\; [0.2 \\;\\cdot\\; V(\\text{Bad}) \\;+\\; 0.8 \\; \\cdot \\;  V(\\text{Normal})], \\;\\; \n",
    "1 \\;+\\; \\frac{1}{1+r} \\;\\cdot\\; [0.1 \\;\\cdot\\; V(\\text{Bad}) \\; + \\; 0.6 \\;\\cdot\\; V(\\text{Normal}) \\; + \\; 0.3 \\; \\cdot \\; V(\\text{Good})]  \\;\\}\n",
    "\\end{aligned}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dc8456a",
   "metadata": {},
   "source": [
    "# Question 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d3714982",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import quantecon as qe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2c68f13b",
   "metadata": {},
   "outputs": [],
   "source": [
    "states = ['Bad', 'Normal', 'Good']\n",
    "controls = ['No Buy', 'Buy']\n",
    "reward= np.array([1,0,2,1,4,3]).reshape(3,2)\n",
    "beta = 0.9\n",
    "value = np.zeros(len(states))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd7e4a0",
   "metadata": {},
   "source": [
    "# Question 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3c7c8220",
   "metadata": {},
   "outputs": [],
   "source": [
    "M_nobuy = np.array([0.8,0.2,0,0.2,0.8,0,0.1,0.4,0.5]).reshape(3,3)\n",
    "M_buy = np.array([0.5,0.5,0,0.1,0.6,0.3,0.1,0.3,0.6]).reshape(3,3)\n",
    "transitions = {'No Buy':M_nobuy, 'Buy':M_buy}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac33e6d",
   "metadata": {},
   "source": [
    "# Question 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3ccb4f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_params(states, controls, reward, beta, value, transitions):\n",
    "    n = len(states)\n",
    "    m = len(controls)\n",
    "    if reward.shape != (n, m):\n",
    "        print('The reward function has the wrong dimensions.')\n",
    "    if len(value) != n:\n",
    "        print('The value function is not the right length')\n",
    "    if len(transitions) != m:\n",
    "        print(\"There is not the requisite number of transition matrices.\")\n",
    "    if not all(transitions[control].shape == (n,n) for control in controls):\n",
    "        print(\"At least one of the Markov transitions is one of the wrong dimension.\")\n",
    "    if not all(np.ones(n)@transitions[control]@np.ones(n) == n for control in controls):\n",
    "        print(\"One of the transitions matrices is not Markov\")\n",
    "    if not all((transitions[control] >= 0).all() and (transitions[control] <= 1).all() for control in controls):\n",
    "        print(\"The elements of the transition matrices are not probabilities.\")\n",
    "    if beta < 0 or beta >= 1:\n",
    "        print(\"Something is wrong with the discount factor.\")\n",
    "    return (print(\"Requisite checks done, all is compatible.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f4ce00aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requisite checks done, all is compatible.\n"
     ]
    }
   ],
   "source": [
    "check_params(states, controls, reward, beta, value, transitions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b8b8634",
   "metadata": {},
   "source": [
    "# Question 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "29229e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transition_probs(state, control, transitions):\n",
    "    row = states.index(state)\n",
    "    tmat = transitions[control]\n",
    "    return tmat[row]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d7acade0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.1, 0.6, 0.3])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transition_probs('Normal', 'Buy', transitions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1acae6ad",
   "metadata": {},
   "source": [
    "# Question 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b88d3a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_value(states, controls, reward, transitions, value, beta):\n",
    "    val_copy = value.copy()\n",
    "    for state in states:\n",
    "        val = []\n",
    "        for control in controls:\n",
    "            payoff = reward[states.index(state), controls.index(control)] + beta * (value @ transition_probs(state, control, transitions))\n",
    "            val.append(payoff)\n",
    "        val_copy[states.index(state)] = np.max(val)\n",
    "    return(val_copy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "be797c34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 2., 4.])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "update_value(states, controls, reward, transitions, value, beta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "691ee036",
   "metadata": {},
   "source": [
    "# Question 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "156f5dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve(states,controls,reward,transitions,value,beta):\n",
    "    epsilon = 1e-6\n",
    "    value_check = value.copy()\n",
    "    max_iter = 500\n",
    "    counter = 0\n",
    "    \n",
    "    while (all(abs(update_value(states, controls, reward, transitions, value_check, beta) - value_check) >= epsilon)) and (counter <= max_iter):\n",
    "        value_check = update_value(states, controls, reward, transitions, value_check, beta)\n",
    "        counter += 1\n",
    "    if counter >=500:\n",
    "        print('Did not converge!')\n",
    "    return(value_check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c41f3d7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([14.80552819, 17.4752711 , 21.13380769])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value = solve(states,controls,reward,transitions,value,beta)\n",
    "value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e276301f",
   "metadata": {},
   "source": [
    "# Question 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9a8ae217",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimal_policy(states, controls, reward, transitions, value, beta):\n",
    "    optimal = []\n",
    "    for state in states:\n",
    "        val = []\n",
    "        for control in controls:\n",
    "            payoff = reward[states.index(state), controls.index(control)] + beta * (value @ transition_probs(state, control, transitions))\n",
    "            val.append(payoff)\n",
    "        optimal.append(controls[np.argmax(val)])\n",
    "    return(optimal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7259ace5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['No Buy', 'Buy', 'No Buy']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "op = optimal_policy(states, controls, reward, transitions, value, beta)\n",
    "op"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e6568e8",
   "metadata": {},
   "source": [
    "# Question 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "de55fc27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transition_matrix_optimal(states, controls, reward, transitions, value, beta):\n",
    "    op = optimal_policy(states, controls, reward, transitions, value, beta)\n",
    "    op_transition_matrix = np.zeros(9).reshape(3,3)\n",
    "    for control in range(len(op)):\n",
    "        op_transition_matrix[control] = transitions[op[control]][control]\n",
    "    return op_transition_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9f6291b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.8, 0.2, 0. ],\n",
       "       [0.1, 0.6, 0.3],\n",
       "       [0.1, 0.4, 0.5]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P = transition_matrix_optimal(states, controls, reward, transitions, value, beta)\n",
    "P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bc433e9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.33333333, 0.41666667, 0.25      ]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mc = qe.MarkovChain(P)\n",
    "p_stationary = mc.stationary_distributions \n",
    "p_stationary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e6510da",
   "metadata": {},
   "source": [
    "# Question 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "fcf6a785",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_8e25e th.col_heading {\n",
       "  text-align: center;\n",
       "}\n",
       "#T_8e25e td {\n",
       "  text-align: center;\n",
       "  font-weight: bold;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_8e25e\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_8e25e_level0_col0\" class=\"col_heading level0 col0\" >Reward if No Buy</th>\n",
       "      <th id=\"T_8e25e_level0_col1\" class=\"col_heading level0 col1\" >Reward if Buy</th>\n",
       "      <th id=\"T_8e25e_level0_col2\" class=\"col_heading level0 col2\" >Solved Value Function</th>\n",
       "      <th id=\"T_8e25e_level0_col3\" class=\"col_heading level0 col3\" >Optimal Policy</th>\n",
       "      <th id=\"T_8e25e_level0_col4\" class=\"col_heading level0 col4\" >Probabilities from Ergodic Distribution</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_8e25e_level0_row0\" class=\"row_heading level0 row0\" >Bad</th>\n",
       "      <td id=\"T_8e25e_row0_col0\" class=\"data row0 col0\" >1</td>\n",
       "      <td id=\"T_8e25e_row0_col1\" class=\"data row0 col1\" >0</td>\n",
       "      <td id=\"T_8e25e_row0_col2\" class=\"data row0 col2\" >14.81</td>\n",
       "      <td id=\"T_8e25e_row0_col3\" class=\"data row0 col3\" >No Buy</td>\n",
       "      <td id=\"T_8e25e_row0_col4\" class=\"data row0 col4\" >0.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_8e25e_level0_row1\" class=\"row_heading level0 row1\" >Normal</th>\n",
       "      <td id=\"T_8e25e_row1_col0\" class=\"data row1 col0\" >2</td>\n",
       "      <td id=\"T_8e25e_row1_col1\" class=\"data row1 col1\" >1</td>\n",
       "      <td id=\"T_8e25e_row1_col2\" class=\"data row1 col2\" >17.48</td>\n",
       "      <td id=\"T_8e25e_row1_col3\" class=\"data row1 col3\" >Buy</td>\n",
       "      <td id=\"T_8e25e_row1_col4\" class=\"data row1 col4\" >0.42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_8e25e_level0_row2\" class=\"row_heading level0 row2\" >Good</th>\n",
       "      <td id=\"T_8e25e_row2_col0\" class=\"data row2 col0\" >4</td>\n",
       "      <td id=\"T_8e25e_row2_col1\" class=\"data row2 col1\" >3</td>\n",
       "      <td id=\"T_8e25e_row2_col2\" class=\"data row2 col2\" >21.13</td>\n",
       "      <td id=\"T_8e25e_row2_col3\" class=\"data row2 col3\" >No Buy</td>\n",
       "      <td id=\"T_8e25e_row2_col4\" class=\"data row2 col4\" >0.25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x2a140bd6af0>"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame({'Reward if No Buy': list(reward[:, 0]), 'Reward if Buy': list(reward[:,1]), 'Solved Value Function': list(np.around(value, 2)), \n",
    "              'Optimal Policy': op, 'Probabilities from Ergodic Distribution': list(np.around(p_stationary[0],2))}, index = states).style.format(\n",
    "                precision=2).set_table_styles([{'selector': 'th.col_heading', 'props': 'text-align: center;'},\n",
    "                                                {'selector': 'td', 'props': 'text-align: center; font-weight: bold;'},], overwrite=False) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1df3f12",
   "metadata": {},
   "source": [
    "# Question 12\n",
    "Creating the class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "96a13270",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DP_Markov():\n",
    "    \n",
    "    def __init__(self, states, controls, reward, beta, value, transitions): #eliminated the need for the check_parameters by including it in the initialization\n",
    "        import numpy as np\n",
    "        import pandas as pd\n",
    "        import quantecon as qe\n",
    "        self.states = states\n",
    "        self.controls = controls\n",
    "        self.reward = reward\n",
    "        self.beta = beta\n",
    "        self.value = value\n",
    "        self.transitions = transitions\n",
    "        n = len(self.states)\n",
    "        m = len(self.controls)\n",
    "        if self.reward.shape != (n, m):\n",
    "            raise Exception(\"Reward Function has incorrect dimensions\")\n",
    "        elif len(self.value) != n:\n",
    "            raise Exception(\"Value Function is not the correct length\")\n",
    "        elif len(self.transitions) != m:\n",
    "            raise Exception(\"Incorrect number of transition matrices\")\n",
    "        elif not all(self.transitions[control].shape == (n,n) for control in self.controls):\n",
    "            raise Exception(\"At least one of the Markov transitions is not the correct dimension\")\n",
    "        elif not all(np.ones(n)@self.transitions[control]@np.ones(n) == n for control in self.controls):\n",
    "            raise Exception(\"At least one of the transitions is not in Markov form\")\n",
    "        elif not all((self.transitions[control] >= 0).all() and (self.transitions[control] <= 1).all() for control in self.controls):\n",
    "            raise Exception(\"At least one elements of the transition matrices are not probabilities between 0 and 1\")\n",
    "        elif self.beta < 0 or self.beta >= 1:\n",
    "            raise Exception(\"The Disxcount factor is not in (0,1]\")\n",
    "        else:\n",
    "            pass\n",
    "    \n",
    "    def __str__(self):\n",
    "        return \"States: \" f'{self.states}' \"\\n Controls: \" f'{self.controls}' \"\\n Reward: \" f'{self.reward}' \"\\n Beta: \" f'{self.beta}' \"\\n Value Function: \" f'{self.value}' \"\\n Transitions: \" f'{self.transitions}' \n",
    "    \n",
    "    def transition_probs(self, state, control):\n",
    "        row = self.states.index(state)\n",
    "        tmat = self.transitions[control]\n",
    "        return tmat[row]\n",
    "    \n",
    "    def update_value(self):\n",
    "        val_copy = self.value.copy()\n",
    "        for state in self.states:\n",
    "            val = []\n",
    "            for control in self.controls:\n",
    "                payoff = self.reward[self.states.index(state), self.controls.index(control)] + self.beta * (self.value @ self.transition_probs(state, control))\n",
    "                val.append(payoff)\n",
    "            val_copy[self.states.index(state)] = np.max(val)\n",
    "        return(val_copy)\n",
    "    \n",
    "    def solve(self):\n",
    "        epsilon = 1e-6\n",
    "        max_iter = 500\n",
    "        counter = 0\n",
    "\n",
    "        while (all(abs(self.update_value() - self.value) >= epsilon)) and (counter <= max_iter):\n",
    "            self.value = self.update_value()\n",
    "            counter += 1\n",
    "        if counter >=500:\n",
    "            print('Did not converge after 500 iterations')\n",
    "        return(self.value)\n",
    "    \n",
    "    def optimal_policy(self):\n",
    "        optimal = []\n",
    "        for state in self.states:\n",
    "            val = []\n",
    "            for control in self.controls:\n",
    "                payoff = self.reward[self.states.index(state), self.controls.index(control)] + self.beta * (self.value @ self.transition_probs(state, control))\n",
    "                val.append(payoff)\n",
    "            optimal.append(self.controls[np.argmax(val)])\n",
    "        return(optimal)\n",
    "    \n",
    "    def transition_matrix_optimal(self):\n",
    "        op = self.optimal_policy()\n",
    "        op_transition_matrix = np.zeros(9).reshape(3,3)\n",
    "        for control in range(len(op)):\n",
    "            op_transition_matrix[control] = self.transitions[op[control]][control]\n",
    "        return op_transition_matrix\n",
    "    \n",
    "    def optimal_ergodic_dist(self):\n",
    "        P = self.transition_matrix_optimal()\n",
    "        mc = qe.MarkovChain(P)\n",
    "        p_stationary = mc.stationary_distributions \n",
    "        return p_stationary\n",
    "    \n",
    "    def summary_df(self):\n",
    "        df = pd.DataFrame({'Reward if No Buy': list(self.reward[:, 0]), 'Reward if Buy': list(self.reward[:,1]), 'Solved Value Function': list(np.around(self.value, 2)), 'Optimal Policy': self.optimal_policy(), 'Probabilities from Ergodic Distribution': list(np.around(self.optimal_ergodic_dist()[0],2))}, index = states)\n",
    "        return df.style.format(precision=2).set_table_styles([{'selector': 'th.col_heading', 'props': 'text-align: center;'},\n",
    "                                                             {'selector': 'td', 'props': 'text-align: center; font-weight: bold;'},], overwrite=False) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30e9bad2",
   "metadata": {},
   "source": [
    "Defining the Parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "75f42afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "states = ['Bad', 'Normal', 'Good']\n",
    "controls = ['No Buy', 'Buy']\n",
    "reward= np.array([1,0,2,1,4,3]).reshape(3,2)\n",
    "beta = 0.9\n",
    "value = np.zeros(len(states))\n",
    "\n",
    "M_nobuy = np.array([0.8,0.2,0,0.2,0.8,0,0.1,0.4,0.5]).reshape(3,3)\n",
    "M_buy = np.array([0.5,0.5,0,0.1,0.6,0.3,0.1,0.3,0.6]).reshape(3,3)\n",
    "transitions = {'No Buy':M_nobuy, 'Buy':M_buy}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cfe5929",
   "metadata": {},
   "source": [
    "Example of Failure Initialization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "17e73df9",
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "Incorrect number of transition matrices",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[1;36m(most recent call last)\u001b[0m:\n",
      "  File \u001b[0;32m\"C:\\Users\\scrog\\AppData\\Local\\Temp\\ipykernel_14936\\3903826213.py\"\u001b[0m, line \u001b[0;32m2\u001b[0m, in \u001b[0;35m<module>\u001b[0m\n    DPclassfail = DP_Markov(states, controls, reward, beta, value, transitionsbad)\n",
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\scrog\\AppData\\Local\\Temp\\ipykernel_14936\\1337994389.py\"\u001b[1;36m, line \u001b[1;32m20\u001b[1;36m, in \u001b[1;35m__init__\u001b[1;36m\u001b[0m\n\u001b[1;33m    raise Exception(\"Incorrect number of transition matrices\")\u001b[0m\n",
      "\u001b[1;31mException\u001b[0m\u001b[1;31m:\u001b[0m Incorrect number of transition matrices\n"
     ]
    }
   ],
   "source": [
    "transitionsbad = {'No Buy':M_nobuy}\n",
    "DPclassfail = DP_Markov(states, controls, reward, beta, value, transitionsbad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a864f60b",
   "metadata": {},
   "source": [
    "Proper Initialization and Display of Parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "ccc4f543",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "States: ['Bad', 'Normal', 'Good']\n",
      " Controls: ['No Buy', 'Buy']\n",
      " Reward: [[1 0]\n",
      " [2 1]\n",
      " [4 3]]\n",
      " Beta: 0.9\n",
      " Value Function: [0. 0. 0.]\n",
      " Transitions: {'No Buy': array([[0.8, 0.2, 0. ],\n",
      "       [0.2, 0.8, 0. ],\n",
      "       [0.1, 0.4, 0.5]]), 'Buy': array([[0.5, 0.5, 0. ],\n",
      "       [0.1, 0.6, 0.3],\n",
      "       [0.1, 0.3, 0.6]])}\n"
     ]
    }
   ],
   "source": [
    "DPclass = DP_Markov(states, controls, reward, beta, value, transitions)\n",
    "print(DPclass)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e8e943",
   "metadata": {},
   "source": [
    "Transition Probabilities Method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "8cf3f67d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.1, 0.4, 0.5])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DPclass.transition_probs('Good', 'No Buy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a2ccc30",
   "metadata": {},
   "source": [
    "Update Value Function Method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "b9fb7ad5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([14.80552909, 17.47527201, 21.13380859])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DPclass.update_value()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc666b6",
   "metadata": {},
   "source": [
    "Solving the Dynamic Program Method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "c379ab33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([14.80552819, 17.4752711 , 21.13380769])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DPclass.solve()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c50ab8",
   "metadata": {},
   "source": [
    "Optimal Policy Method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "ea425acc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['No Buy', 'Buy', 'No Buy']"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DPclass.optimal_policy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cf843b3",
   "metadata": {},
   "source": [
    "Optimal Transition Matrix Method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "ca25dd41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.8, 0.2, 0. ],\n",
       "       [0.1, 0.6, 0.3],\n",
       "       [0.1, 0.4, 0.5]])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DPclass.transition_matrix_optimal()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30cca54e",
   "metadata": {},
   "source": [
    "Optimal Ergodic Distribution Method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "6389c8bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.33333333, 0.41666667, 0.25      ]])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DPclass.optimal_ergodic_dist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69874223",
   "metadata": {},
   "source": [
    "Summary Table Method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "0473d84f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_90106 th.col_heading {\n",
       "  text-align: center;\n",
       "}\n",
       "#T_90106 td {\n",
       "  text-align: center;\n",
       "  font-weight: bold;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_90106\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_90106_level0_col0\" class=\"col_heading level0 col0\" >Reward if No Buy</th>\n",
       "      <th id=\"T_90106_level0_col1\" class=\"col_heading level0 col1\" >Reward if Buy</th>\n",
       "      <th id=\"T_90106_level0_col2\" class=\"col_heading level0 col2\" >Solved Value Function</th>\n",
       "      <th id=\"T_90106_level0_col3\" class=\"col_heading level0 col3\" >Optimal Policy</th>\n",
       "      <th id=\"T_90106_level0_col4\" class=\"col_heading level0 col4\" >Probabilities from Ergodic Distribution</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_90106_level0_row0\" class=\"row_heading level0 row0\" >Bad</th>\n",
       "      <td id=\"T_90106_row0_col0\" class=\"data row0 col0\" >1</td>\n",
       "      <td id=\"T_90106_row0_col1\" class=\"data row0 col1\" >0</td>\n",
       "      <td id=\"T_90106_row0_col2\" class=\"data row0 col2\" >14.81</td>\n",
       "      <td id=\"T_90106_row0_col3\" class=\"data row0 col3\" >No Buy</td>\n",
       "      <td id=\"T_90106_row0_col4\" class=\"data row0 col4\" >0.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_90106_level0_row1\" class=\"row_heading level0 row1\" >Normal</th>\n",
       "      <td id=\"T_90106_row1_col0\" class=\"data row1 col0\" >2</td>\n",
       "      <td id=\"T_90106_row1_col1\" class=\"data row1 col1\" >1</td>\n",
       "      <td id=\"T_90106_row1_col2\" class=\"data row1 col2\" >17.48</td>\n",
       "      <td id=\"T_90106_row1_col3\" class=\"data row1 col3\" >Buy</td>\n",
       "      <td id=\"T_90106_row1_col4\" class=\"data row1 col4\" >0.42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_90106_level0_row2\" class=\"row_heading level0 row2\" >Good</th>\n",
       "      <td id=\"T_90106_row2_col0\" class=\"data row2 col0\" >4</td>\n",
       "      <td id=\"T_90106_row2_col1\" class=\"data row2 col1\" >3</td>\n",
       "      <td id=\"T_90106_row2_col2\" class=\"data row2 col2\" >21.13</td>\n",
       "      <td id=\"T_90106_row2_col3\" class=\"data row2 col3\" >No Buy</td>\n",
       "      <td id=\"T_90106_row2_col4\" class=\"data row2 col4\" >0.25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x2a13f794730>"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DPclass.summary_df()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Spyder)",
   "language": "python3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
